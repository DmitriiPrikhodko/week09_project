# --------------------------
# 1. –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –∏ –∏–º–ø–æ—Ä—Ç—ã
# --------------------------
import os
from main import *
import numpy as np
import pandas as pd
import streamlit as st
import plotly.graph_objects as go
import torch
from PIL import Image
import requests
from io import BytesIO
import time
from torchvision import transforms as T
from resources.model_unet.model_unet import UNet
import matplotlib.pyplot as plt
import io

DEVICE = torch.device("cpu")
current_dir = os.path.dirname(__file__)
project_root = os.path.join(current_dir, "..")

st.set_page_config(page_title="Images AI processing", layout="wide")
st.sidebar.markdown("### –ü–µ—Ä–µ–π—Ç–∏ –∫ —Å—Ç—Ä–∞–Ω–∏—Ü–µ:")
st.sidebar.page_link("main.py", label="üè† –ì–ª–∞–≤–Ω–∞—è")
st.sidebar.page_link("pages/Faces.py", label="üë§ –î–µ—Ç–µ–∫—Ü–∏—è –ª–∏—Ü")
st.sidebar.page_link("pages/Tumors.py", label="üß† –î–µ—Ç–µ–∫—Ü–∏—è –æ–ø—É—Ö–æ–ª–µ–π –º–æ–∑–≥–∞")
st.sidebar.page_link("pages/Forest.py", label="üå≤ –°–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è –ª–µ—Å–æ–≤")
st.title("–†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ –ª–µ—Å–æ–≤ –Ω–∞ —Å–ø—É—Ç–Ω–∏–∫–æ–≤—ã—Ö —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏—è—Ö")

# --------------------------
# 2. –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
# --------------------------
WEIGHTS_DIR = os.path.join(project_root, "resources/model_unet")
WEIGHTS_PATH = os.path.join(WEIGHTS_DIR, "unet_weights_2.pth")


@st.cache_resource
def load_model():
    os.makedirs(WEIGHTS_DIR, exist_ok=True)
    if not os.path.exists(WEIGHTS_PATH):
        public_url = "https://disk.yandex.ru/d/2dijVvxAxH8WNg"
        r = requests.get(
            "https://cloud-api.yandex.net/v1/disk/public/resources/download",
            params={"public_key": public_url},
        )
        r.raise_for_status()
        download_url = r.json()["href"]
        with requests.get(download_url, stream=True) as response:
            response.raise_for_status()
            with open(WEIGHTS_PATH, "wb") as f:
                for chunk in response.iter_content(chunk_size=8192):
                    f.write(chunk)
    model = UNet(n_class=2)
    model.load_state_dict(torch.load(WEIGHTS_PATH, map_location="cpu"))
    model.eval()
    return model


model = load_model()
model.to(DEVICE)


# --------------------------
# 3. –§—É–Ω–∫—Ü–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
# --------------------------
def get_prediction(
    model, img, device="cpu", img_size=(256, 256), overlay_color=(0, 0, 255), alpha=0.4
):
    model.eval()
    with torch.no_grad():
        orig_size = img.size
        transform = T.Compose([T.Resize(img_size), T.ToTensor()])
        img_tensor = transform(img).unsqueeze(0).to(device)

        output = model(img_tensor)
        if output.shape[1] == 1:
            mask = torch.sigmoid(output)
            mask = (mask > 0.5).float()
        else:
            mask = torch.argmax(output, dim=1, keepdim=True).float()
        mask = mask.squeeze(0).squeeze(0).cpu().numpy()

        mask_img = Image.fromarray((mask * 255).astype(np.uint8)).resize(orig_size)
        mask_np = np.array(mask_img) / 255.0

        overlay = np.zeros((orig_size[1], orig_size[0], 3), dtype=np.uint8)
        overlay[..., 0] = overlay_color[0]
        overlay[..., 1] = overlay_color[1]
        overlay[..., 2] = overlay_color[2]

        img_np = np.array(img).astype(np.uint8)
        combined_np = (
            img_np * (1 - alpha * mask_np[..., None])
            + overlay * (alpha * mask_np[..., None])
        ).astype(np.uint8)
        combined_img = Image.fromarray(combined_np)

    return mask_img, combined_img


# --------------------------
# 4. –ó–∞–≥—Ä—É–∑–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –æ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
# --------------------------
st.subheader("–ó–∞–≥—Ä—É–∑–∏—Ç–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏–ª–∏ –≤—Å—Ç–∞–≤—å—Ç–µ —Å—Å—ã–ª–∫–∏")
col1, col2 = st.columns(2)
with col1:
    uploaded_files = st.file_uploader(
        "üìÅ –ó–∞–≥—Ä—É–∑–∏—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è",
        type=["jpg", "jpeg", "png"],
        accept_multiple_files=True,
    )
with col2:
    urls_input = st.text_area(
        "üåê –í—Å—Ç–∞–≤—å—Ç–µ —Å—Å—ã–ª–∫–∏ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è (–∫–∞–∂–¥—É—é —Å –Ω–æ–≤–æ–π —Å—Ç—Ä–æ–∫–∏)"
    )

images = []
if uploaded_files:
    for f in uploaded_files:
        try:
            img = Image.open(f).convert("RGB")
            images.append(("–§–∞–π–ª: " + f.name, img))
        except Exception as e:
            st.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ç–∫—Ä—ã—Ç—å —Ñ–∞–π–ª {f.name}: {e}")
if urls_input:
    urls = [u.strip() for u in urls_input.split("\n") if u.strip()]
    for idx, url in enumerate(urls):
        try:
            response = requests.get(url)
            img = Image.open(BytesIO(response.content)).convert("RGB")
            images.append((f"–°—Å—ã–ª–∫–∞ {idx+1}", img))
        except Exception as e:
            st.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –ø–æ —Å—Å—ã–ª–∫–µ {url}: {e}")

# --------------------------
# 5. –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –∏ –≤—ã–≤–æ–¥ —Å –∫–Ω–æ–ø–∫–∞–º–∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è
# --------------------------
if images:
    st.subheader("–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏")
    cols_per_row = 4
    for i in range(0, len(images), cols_per_row):
        cols = st.columns(cols_per_row)
        for j, (name, image) in enumerate(images[i : i + cols_per_row]):
            with cols[j]:
                start_time = time.time()
                mask_img, combined_img = get_prediction(model, image)
                st.image(
                    [image, combined_img],
                    width=200,
                    caption=[name, "–°–µ–≥–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —Ñ–æ—Ç–æ"],
                )
                end_time = time.time()
                st.info(f"‚è± {end_time-start_time:.3f} —Å–µ–∫")

                # –ö–Ω–æ–ø–∫–∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è
                buf_mask = io.BytesIO()
                mask_img.save(buf_mask, format="PNG")
                buf_mask.seek(0)
                st.download_button(
                    f"üì• –°–∫–∞—á–∞—Ç—å –º–∞—Å–∫—É {name}",
                    buf_mask,
                    file_name=f"mask_{name}.png",
                    mime="image/png",
                )

                buf_combined = io.BytesIO()
                combined_img.save(buf_combined, format="PNG")
                buf_combined.seek(0)
                st.download_button(
                    f"üì• –°–∫–∞—á–∞—Ç—å —Å–µ–≥–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —Ñ–æ—Ç–æ {name}",
                    buf_combined,
                    file_name=f"segmented_{name}.png",
                    mime="image/png",
                )

# --------------------------
# 6. –û–ø–∏—Å–∞–Ω–∏–µ –ø—Ä–æ—Ü–µ—Å—Å–∞ –æ–±—É—á–µ–Ω–∏—è
# --------------------------
st.markdown("## üß† –ü—Ä–æ—Ü–µ—Å—Å –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏")
st.markdown(
    """
    <div style="font-size:20px; line-height:1.6;">
    –î–ª—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∞—Å—å –º–æ–¥–µ–ª—å 
    <a href="https://towardsdatascience.com/cook-your-first-u-net-in-pytorch-b3297a844cf3/" target="_blank">Unet</a> 
    —Å 2 –∫–ª–∞—Å—Å–∞–º–∏ (–õ–µ—Å / –û—Å—Ç–∞–ª—å–Ω–æ–µ)<br><br>

    –î–ª—è –æ–±—É—á–µ–Ω–∏—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª—Å—è 
    <a href="https://www.kaggle.com/datasets/quadeer15sh/augmented-forest-segmentation" target="_blank">–¥–∞—Ç–∞—Å–µ—Ç</a>
    —Å –∞—ç—Ä–æ–∫–æ—Å–º–∏—á–µ—Å–∫–∏–º–∏ —Å–Ω–∏–º–∫–∞–º–∏<br><br>

    - –î–∞—Ç–∞—Å–µ—Ç —Å–æ—Å—Ç–æ—è–ª –∏–∑ <strong>5108</strong> —Å–Ω–∏–º–∫–æ–≤ –∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏—Ö –º–∞—Å–æ–∫<br>
    - –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ: <strong>80%</strong> –Ω–∞ –æ–±—É—á–µ–Ω–∏–µ, <strong>20%</strong> –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏—é<br>
    - –û–±—É—á–µ–Ω–∏–µ: <strong>10 —ç–ø–æ—Ö</strong> (~25 –º–∏–Ω—É—Ç)<br>
    - –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ <strong>20 —ç–ø–æ—Ö–∞—Ö</strong> –Ω–µ –¥–∞–ª–∞ –∑–Ω–∞—á–∏–º–æ–≥–æ —É–ª—É—á—à–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫ –ø–æ—Å–ª–µ 10-–π<br><br>

    –ù–∏–∂–µ –ø—Ä–∏–≤–µ–¥–µ–Ω—ã —Ñ–∏–Ω–∞–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –∏ –≥—Ä–∞—Ñ–∏–∫–∏ –∏—Ö —ç–≤–æ–ª—é—Ü–∏–∏ –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è:
    </div>
    """,
    unsafe_allow_html=True,
)

# --------------------------
# 7. –ß—Ç–µ–Ω–∏–µ CSV –∏ –≥—Ä–∞—Ñ–∏–∫–∏ Plotly
# --------------------------
METRICS_PATH = os.path.join(project_root, "resources/model_unet/metrics")
metrics_files = {
    "Loss": (
        os.path.join(METRICS_PATH, "train_loss.csv"),
        os.path.join(METRICS_PATH, "valid_loss.csv"),
    ),
    "Accuracy": (
        os.path.join(METRICS_PATH, "train_accuracy.csv"),
        os.path.join(METRICS_PATH, "valid_accuracy.csv"),
    ),
    "Dice": (
        os.path.join(METRICS_PATH, "train_dice.csv"),
        os.path.join(METRICS_PATH, "valid_dice.csv"),
    ),
    "IoU": (
        os.path.join(METRICS_PATH, "train_iou.csv"),
        os.path.join(METRICS_PATH, "valid_iou.csv"),
    ),
}

# –¢–∞–±–ª–∏—Ü–∞ –∏—Ç–æ–≥–æ–≤—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
summary_data = []
for metric_name, (train_file, val_file) in metrics_files.items():
    try:
        df_train = pd.read_csv(train_file)
        df_val = pd.read_csv(val_file)
        last_train = round(df_train["value"].iloc[-1], 3)
        last_val = round(df_val["value"].iloc[-1], 3)
        summary_data.append(
            {"–ú–µ—Ç—Ä–∏–∫–∞": metric_name, "Train": last_train, "Valid": last_val}
        )
    except FileNotFoundError:
        summary_data.append({"–ú–µ—Ç—Ä–∏–∫–∞": metric_name, "Train": "-", "Valid": "-"})

summary_df = pd.DataFrame(summary_data)


def highlight_columns(x):
    df = x.copy()
    df["Train"] = "background-color: #d0f0c0"
    df["Valid"] = "background-color: #add8e6"
    df["–ú–µ—Ç—Ä–∏–∫–∞"] = ""
    return df


styled_df = summary_df.style.apply(highlight_columns, axis=None).format(
    "{:.3f}", subset=["Train", "Valid"]
)
col1, col2 = st.columns([1, 1])
with col1:
    st.subheader("üìä –ò—Ç–æ–≥–æ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫ (–ø–æ—Å–ª–µ–¥–Ω—è—è —ç–ø–æ—Ö–∞)")
    st.dataframe(styled_df, use_container_width=True)

# –ì—Ä–∞—Ñ–∏–∫–∏ Plotly
col1, col2 = st.columns(2)
for idx, (metric_name, (train_file, val_file)) in enumerate(metrics_files.items()):
    try:
        df_train = pd.read_csv(train_file)
        df_val = pd.read_csv(val_file)
        fig = go.Figure()
        fig.add_trace(
            go.Scatter(
                x=df_train["step"],
                y=df_train["value"],
                mode="lines+markers",
                name="train",
            )
        )
        fig.add_trace(
            go.Scatter(
                x=df_val["step"], y=df_val["value"], mode="lines+markers", name="valid"
            )
        )
        fig.update_layout(
            title=f"{metric_name} –ø–æ —ç–ø–æ—Ö–∞–º",
            xaxis_title="–≠–ø–æ—Ö–∞",
            yaxis_title=metric_name,
            template="plotly_white",
            title_x=0.5,
            margin=dict(l=20, r=20, t=50, b=20),
            legend=dict(title=""),
        )
        if idx % 2 == 0:
            col1.plotly_chart(fig, use_container_width=True)
        else:
            col2.plotly_chart(fig, use_container_width=True)
    except FileNotFoundError:
        st.warning(
            f"‚ö†Ô∏è –§–∞–π–ª {train_file} –∏–ª–∏ {val_file} –Ω–µ –Ω–∞–π–¥–µ–Ω. –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –≥—Ä–∞—Ñ–∏–∫ {metric_name}."
        )
